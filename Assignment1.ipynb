{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE7455 Assignment 1\n",
    "Peng Hongyi (G2105029E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the provided code at first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: torch.Size([2088628]), Val: torch.Size([217646]), Test: torch.Size([245569])\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data/wikitext-2'\n",
    "corpus = data.Corpus(data_dir)\n",
    "print(f'Train: {corpus.train.shape}, Val: {corpus.valid.shape}, Test: {corpus.test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "EVAL_BATCH_SIZE = 10\n",
    "train_data = batchify(corpus.train, BATCH_SIZE)\n",
    "val_data = batchify(corpus.valid, EVAL_BATCH_SIZE)\n",
    "test_data = batchify(corpus.test, EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOKENS = len(corpus.dictionary)\n",
    "import model\n",
    "MODEL = \"LSTM\"\n",
    "EMSIZE = 200\n",
    "N_HID = 200\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "TIED = \"store_true\"\n",
    "LR = 20\n",
    "CLIP_TH = 0.25\n",
    "model = model.RNNModel(MODEL, N_TOKENS, EMSIZE, N_HID, N_LAYERS, DROPOUT, TIED).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPTT = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(BPTT, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 20])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, _ = get_batch(train_data, 0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |   200/ 2983 batches | lr 20.00 | ms/batch 14.17 | loss  5.85 | ppl   346.21\n",
      "| epoch   0 |   400/ 2983 batches | lr 20.00 | ms/batch 14.28 | loss  5.73 | ppl   308.45\n",
      "| epoch   0 |   600/ 2983 batches | lr 20.00 | ms/batch 14.51 | loss  5.54 | ppl   255.88\n",
      "| epoch   0 |   800/ 2983 batches | lr 20.00 | ms/batch 14.33 | loss  5.49 | ppl   243.37\n",
      "| epoch   0 |  1000/ 2983 batches | lr 20.00 | ms/batch 14.34 | loss  5.45 | ppl   231.66\n",
      "| epoch   0 |  1200/ 2983 batches | lr 20.00 | ms/batch 14.08 | loss  5.43 | ppl   228.43\n",
      "| epoch   0 |  1400/ 2983 batches | lr 20.00 | ms/batch 14.19 | loss  5.47 | ppl   236.80\n",
      "| epoch   0 |  1600/ 2983 batches | lr 20.00 | ms/batch 14.09 | loss  5.62 | ppl   274.80\n",
      "| epoch   0 |  1800/ 2983 batches | lr 20.00 | ms/batch 14.03 | loss  5.48 | ppl   238.77\n",
      "| epoch   0 |  2000/ 2983 batches | lr 20.00 | ms/batch 14.17 | loss  5.47 | ppl   238.45\n",
      "| epoch   0 |  2200/ 2983 batches | lr 20.00 | ms/batch 14.16 | loss  5.38 | ppl   217.58\n",
      "| epoch   0 |  2400/ 2983 batches | lr 20.00 | ms/batch 14.26 | loss  5.42 | ppl   225.39\n",
      "| epoch   0 |  2600/ 2983 batches | lr 20.00 | ms/batch 14.38 | loss  5.41 | ppl   224.13\n",
      "| epoch   0 |  2800/ 2983 batches | lr 20.00 | ms/batch 14.32 | loss  5.32 | ppl   204.65\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import time\n",
    "import math\n",
    "criterion = nn.NLLLoss()\n",
    "model.train()\n",
    "total_loss = 0\n",
    "start_time = time.time()\n",
    "hidden = model.init_hidden(BATCH_SIZE)\n",
    "for epoch in range(1):\n",
    "    for batch, i in enumerate(range(0, train_data.size(0)-1, BPTT)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        model.zero_grad()\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), CLIP_TH)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(p.grad, alpha=-LR)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % 200 == 0 and batch > 0:\n",
    "            cur_loss = total_loss / 200\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // BPTT, LR,\n",
    "                elapsed * 1000 / 200, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write my own FNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNNModel(nn.Module):\n",
    "    def __init__(self, n_token, n_emb, n_hidden, bptt):\n",
    "        super().__init__()\n",
    "        self.n_token = n_token\n",
    "        self.n_emb = n_emb \n",
    "        self.n_hidden = n_hidden\n",
    "        self.bptt = bptt\n",
    "        self.encoder = nn.Embedding(n_token, n_emb)\n",
    "        self.hidden = nn.Linear(n_emb*bptt, n_hidden)\n",
    "        self.decoder = nn.Linear(n_hidden, n_token)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        emb = self.encoder(input)\n",
    "        emb = torch.cat(emb, dim=1)\n",
    "        emb = nn.Tanh(emb)\n",
    "        assert emb.shape[1] == self.n_dim*self.bptt\n",
    "        out = self.hidden(emb)\n",
    "        decoded = self.decoder(out)\n",
    "        decoded = decoded.view(-1, self.n_token)\n",
    "        decoded = nn.Softmax(decoded)\n",
    "        return decoded\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 35]) torch.Size([700])\n",
      "tensor([[    0,     1,     2,     3,     4,     1,     0,     0,     5,     6,\n",
      "             2,     7,     8,     9,     3,    10,    11,     8,    12,    13,\n",
      "            14,    15,     2,    16,    17,    18,     7,    19,    13,    20,\n",
      "            21,    22,    23,     2,     3],\n",
      "        [  284,   357,  1496,   449,  5181,    13,    17,  1207,  1870,    43,\n",
      "          1809,    13, 10314,   144,    27,  1426,    30,  3022,   910,  4781,\n",
      "            15,    83, 10539, 10540,  2191,    17,   669,   831, 10518,    93,\n",
      "           828,  1721,    13,  3334,    13],\n",
      "        [15178,    43,  7369,   310, 15182,    15,   652, 15183,    13,    46,\n",
      "          1104,    17,  3803,  3522,    16, 10525,  3773,    13,    37,    43,\n",
      "         15184,    46,   131,   677,    15,    83,  1723,  1109, 15185,  3033,\n",
      "         10525,    43,  6936,   440,    35],\n",
      "        [  280,  2977,   115,     9, 18712,    22, 17400, 18712,    93,  1775,\n",
      "          1908,    15,    83,  2839,   740,  9989,   131,  2585,  1179,   998,\n",
      "            43,    10,  7428,  1179,  1326,  3890,    19,   564,    13,   423,\n",
      "            22,  1721,    43,    10,  4144],\n",
      "        [  348,   530,  4782,    13,   877, 17314,   115, 13108,    37,  4589,\n",
      "          2550,    15,     0,     0,     1,     1, 11071,     1,     1,     0,\n",
      "             0,  6462,   128,  5158, 12169,   257,    16,    17,  5158,    16,\n",
      "           568,  5764,    13,   766, 16388],\n",
      "        [  128, 23080,    37,  8034,    16,   357,  3220,  2004,    13, 23079,\n",
      "         18202,    13,    43,    86,    15, 18202,   131,    17,    59,   578,\n",
      "          2004,    22,  5935,    17,  2015,    43,  1908,  1193,    15,     0,\n",
      "            83,   278,  3822,  8034,   131],\n",
      "        [  289,    13, 22196,  3107,   423,  1517,    59,   271,    13,   162,\n",
      "         22196,   128, 22179,   131, 13574,    93,    47,   271,    39,    27,\n",
      "          2923,    13,    37,    17, 24277,   664,  1324,    22, 21955,    17,\n",
      "          1665,  1372,    15,    83,  1665],\n",
      "        [ 9493,    78,   252,   639,    22,   209,    61,  1100,    16,  6659,\n",
      "          4318,    61,    15,     9,   131,  9629,    35,    27,  6670,    43,\n",
      "            17,  8278,    37, 12376,  6714,  7121,  8502,    15,  7402,   792,\n",
      "            13,  5788,   899,    78,  8180],\n",
      "        [   16,    17, 26998,    13,   562,  2156,    13,    37,  2751, 26999,\n",
      "           321,    78, 23659,   221,   452,    13,   329,   707,   479,  2576,\n",
      "           115,  1009,    43,    17,   289,    15,   135,    26,   253,    43,\n",
      "           159,     9,    43,  2183,  5151],\n",
      "        [    1,     0,     0, 27958,  1575,   348,   530,    17,  1760,    13,\n",
      "          8033,    13,   310,  4201,  1828,    17,  4644,    16, 27977,     9,\n",
      "            13,  5980,    16, 20166,    37,    17, 11528,    10,  1984, 27329,\n",
      "            19,    15,   652,     9,    13],\n",
      "        [   13,  4312, 28680,   638,   496,   131, 15801,    61,    17,  6594,\n",
      "            61,    39,    27,  7615,  3840,    15,     0,     0,     1,     1,\n",
      "             1,     9,    93,    83, 19900,     1,     1,     1,     0,     0,\n",
      "           123,  7998,  3746,    16,    17],\n",
      "        [    0,     0,     1,     1,     1,  2196,  1110,    72,    37,  7781,\n",
      "             1,     1,     1,     0,     0, 14802,  5594,   944, 29432,  3409,\n",
      "            43,  4848,    13,  6385,    22,  5935,  5980,   128,  3409,    13,\n",
      "         12890,   766,   151,   152,    15],\n",
      "        [ 2701,   151,   496,   168,   209,   146,    22,  1218,    17,  4176,\n",
      "            39,   423,    22,  3243,  2931,   826,  6308,    17,    61,   993,\n",
      "            61,    37,    61,   946,    61,  5774,    15, 12890,   128,  9824,\n",
      "          2057,     8,    61,  1344,  8877],\n",
      "        [ 1227,    22,  2193,    17,  1056, 16561,  3168,    52,    22,  2021,\n",
      "            39,    54,    33,    80, 30616,    15,   652,    17,    59,  3102,\n",
      "          7359,    80,    17,  3842,  1237, 29681,    13,    17,  1056, 16561,\n",
      "           131,  6870, 10421,  1839,  2376],\n",
      "        [ 1563, 18215,  1037,    43,    17,   188,   189,    48, 27572,     8,\n",
      "         20503,  1115,    15,  1923,  7810,  5102,    93,   361,  7935,   119,\n",
      "         25670, 31219, 28537,   128,   578, 14840,    13,    37,   361,  4336,\n",
      "          4364,   119,    17,  1134,  1520],\n",
      "        [ 4044,    17,     9,  2786,    39, 31575,    15,  2853,    17, 31575,\n",
      "           131, 14584,    43,  1129,   651,    16,    17,  9170,    35,    17,\n",
      "            52, 16703,  4075,    13,  1754,    17,     9,    37,     9, 30155,\n",
      "            13,    46,   131,   808,   538],\n",
      "        [  115,    17,  4072,    15,   317,   348,    99,   530,  5969,    22,\n",
      "          7107,   284,   214,   225,   119, 13102,  2137,    15,  5883,    13,\n",
      "            17, 17570,   300, 18997,    37,  3122,   227,    17,   460,    35,\n",
      "            61,  4500, 13656,    61,    13],\n",
      "        [ 1352,    46,   380,   160, 19914,    54,  8992,    13,  4683,    46,\n",
      "            23,    46,   380,   119,    17,  5048,    16,   225, 27949,    43,\n",
      "            17, 24897, 11319,    37, 30532,   278,   722,    27,    47,    16,\n",
      "            17,  4235,    16,    17, 11319],\n",
      "        [ 1335,    43,    27,   152,   128,    52,    15,    61, 32643,  3241,\n",
      "          4730,    30,  2054,   704,    15,     0,     0,     1,     1, 29289,\n",
      "          7957,     1,     1,     0,     0,     0,     1,     1,     1,  4724,\n",
      "          1839,  1084,     1,     1,     1],\n",
      "        [   16,  2015, 33001,  3072,   348,   630,   529,   496,   530,  5234,\n",
      "          1857,   440,    83, 12901,    26,    27,  6091,    16,    56,   496,\n",
      "          3072,   348,   186,    13,   119,    54,  1022, 12672,    37,    54,\n",
      "         28100,  2143,    13,   162,  8276]], device='cuda:0')\n",
      "tensor([    1,   357,    43,  2977,   530, 23080,    13,    78,    17,     0,\n",
      "         4312,     0,   151,    22, 18215,    17,    17,    46,    43,  2015,\n",
      "            2,  1496,  7369,   115,  4782,    37, 22196,   252, 26998,     0,\n",
      "        28680,     1,   496,  2193,  1037,     9,  4072,   380,    27, 33001,\n",
      "            3,   449,   310,     9,    13,  8034,  3107,   639,    13, 27958,\n",
      "          638,     1,   168,    17,    43,  2786,    15,   160,   152,  3072,\n",
      "            4,  5181, 15182, 18712,   877,    16,   423,    22,   562,  1575,\n",
      "          496,     1,   209,  1056,    17,    39,   317, 19914,   128,   348,\n",
      "            1,    13,    15,    22, 17314,   357,  1517,   209,  2156,   348,\n",
      "          131,  2196,   146, 16561,   188, 31575,   348,    54,    52,   630,\n",
      "            0,    17,   652, 17400,   115,  3220,    59,    61,    13,   530,\n",
      "        15801,  1110,    22,  3168,   189,    15,    99,  8992,    15,   529,\n",
      "            0,  1207, 15183, 18712, 13108,  2004,   271,  1100,    37,    17,\n",
      "           61,    72,  1218,    52,    48,  2853,   530,    13,    61,   496,\n",
      "            5,  1870,    13,    93,    37,    13,    13,    16,  2751,  1760,\n",
      "           17,    37,    17,    22, 27572,    17,  5969,  4683, 32643,   530,\n",
      "            6,    43,    46,  1775,  4589, 23079,   162,  6659, 26999,    13,\n",
      "         6594,  7781,  4176,  2021,     8, 31575,    22,    46,  3241,  5234,\n",
      "            2,  1809,  1104,  1908,  2550, 18202, 22196,  4318,   321,  8033,\n",
      "           61,     1,    39,    39, 20503,   131,  7107,    23,  4730,  1857,\n",
      "            7,    13,    17,    15,    15,    13,   128,    61,    78,    13,\n",
      "           39,     1,   423,    54,  1115, 14584,   284,    46,    30,   440,\n",
      "            8, 10314,  3803,    83,     0,    43, 22179,    15, 23659,   310,\n",
      "           27,     1,    22,    33,    15,    43,   214,   380,  2054,    83,\n",
      "            9,   144,  3522,  2839,     0,    86,   131,     9,   221,  4201,\n",
      "         7615,     0,  3243,    80,  1923,  1129,   225,   119,   704, 12901,\n",
      "            3,    27,    16,   740,     1,    15, 13574,   131,   452,  1828,\n",
      "         3840,     0,  2931, 30616,  7810,   651,   119,    17,    15,    26,\n",
      "           10,  1426, 10525,  9989,     1, 18202,    93,  9629,    13,    17,\n",
      "           15, 14802,   826,    15,  5102,    16, 13102,  5048,     0,    27,\n",
      "           11,    30,  3773,   131, 11071,   131,    47,    35,   329,  4644,\n",
      "            0,  5594,  6308,   652,    93,    17,  2137,    16,     0,  6091,\n",
      "            8,  3022,    13,  2585,     1,    17,   271,    27,   707,    16,\n",
      "            0,   944,    17,    17,   361,  9170,    15,   225,     1,    16,\n",
      "           12,   910,    37,  1179,     1,    59,    39,  6670,   479, 27977,\n",
      "            1, 29432,    61,    59,  7935,    35,  5883, 27949,     1,    56,\n",
      "           13,  4781,    43,   998,     0,   578,    27,    43,  2576,     9,\n",
      "            1,  3409,   993,  3102,   119,    17,    13,    43, 29289,   496,\n",
      "           14,    15, 15184,    43,     0,  2004,  2923,    17,   115,    13,\n",
      "            1,    43,    61,  7359, 25670,    52,    17,    17,  7957,  3072,\n",
      "           15,    83,    46,    10,  6462,    22,    13,  8278,  1009,  5980,\n",
      "            9,  4848,    37,    80, 31219, 16703, 17570, 24897,     1,   348,\n",
      "            2, 10539,   131,  7428,   128,  5935,    37,    37,    43,    16,\n",
      "           93,    13,    61,    17, 28537,  4075,   300, 11319,     1,   186,\n",
      "           16, 10540,   677,  1179,  5158,    17,    17, 12376,    17, 20166,\n",
      "           83,  6385,   946,  3842,   128,    13, 18997,    37,     0,    13,\n",
      "           17,  2191,    15,  1326, 12169,  2015, 24277,  6714,   289,    37,\n",
      "        19900,    22,    61,  1237,   578,  1754,    37, 30532,     0,   119,\n",
      "           18,    17,    83,  3890,   257,    43,   664,  7121,    15,    17,\n",
      "            1,  5935,  5774, 29681, 14840,    17,  3122,   278,     0,    54,\n",
      "            7,   669,  1723,    19,    16,  1908,  1324,  8502,   135, 11528,\n",
      "            1,  5980,    15,    13,    13,     9,   227,   722,     1,  1022,\n",
      "           19,   831,  1109,   564,    17,  1193,    22,    15,    26,    10,\n",
      "            1,   128, 12890,    17,    37,    37,    17,    27,     1, 12672,\n",
      "           13, 10518, 15185,    13,  5158,    15, 21955,  7402,   253,  1984,\n",
      "            0,  3409,   128,  1056,   361,     9,   460,    47,     1,    37,\n",
      "           20,    93,  3033,   423,    16,     0,    17,   792,    43, 27329,\n",
      "            0,    13,  9824, 16561,  4336, 30155,    35,    16,  4724,    54,\n",
      "           21,   828, 10525,    22,   568,    83,  1665,    13,   159,    19,\n",
      "          123, 12890,  2057,   131,  4364,    13,    61,    17,  1839, 28100,\n",
      "           22,  1721,    43,  1721,  5764,   278,  1372,  5788,     9,    15,\n",
      "         7998,   766,     8,  6870,   119,    46,  4500,  4235,  1084,  2143,\n",
      "           23,    13,  6936,    43,    13,  3822,    15,   899,    43,   652,\n",
      "         3746,   151,    61, 10421,    17,   131, 13656,    16,     1,    13,\n",
      "            2,  3334,   440,    10,   766,  8034,    83,    78,  2183,     9,\n",
      "           16,   152,  1344,  1839,  1134,   808,    61,    17,     1,   162,\n",
      "            3,    13,    35,  4144, 16388,   131,  1665,  8180,  5151,    13,\n",
      "           17,    15,  8877,  2376,  1520,   538,    13, 11319,     1,  8276,\n",
      "            4,  1828,  1756,  1179,    37,   826,  4137,    93,    16,    17,\n",
      "          545,   697,  4453,    15,  8615,    43,  9254,    15,     0,  2313],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = FNNModel(\n",
    "    n_token=N_TOKENS,\n",
    "    n_emb=200,\n",
    "    n_hidden=200,\n",
    "    bptt = 35\n",
    ")\n",
    "criterion = nn.NLLLoss()\n",
    "model.train()\n",
    "total_loss = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1):\n",
    "    for batch, i in enumerate(range(0, train_data.size(0)-1, BPTT)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        model.zero_grad()\n",
    "        data, targets = data.T, targets.T\n",
    "        print(data.shape, targets.shape)\n",
    "        print(data)\n",
    "        print(targets)\n",
    "        break\n",
    "        # output, = model(data, hidden)\n",
    "        # loss = criterion(output, targets)\n",
    "        # loss.backward()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1579d4a348c2ef16482c05d3cfac916f73c8945ddf1938a1e045b3bdea82eece"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('VFL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE7455 Assignment 1\n",
    "Peng Hongyi (G2105029E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the provided code at first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: torch.Size([2088628]), Val: torch.Size([217646]), Test: torch.Size([245569])\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data/wikitext-2'\n",
    "corpus = data.Corpus(data_dir)\n",
    "print(f'Train: {corpus.train.shape}, Val: {corpus.valid.shape}, Test: {corpus.test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "EVAL_BATCH_SIZE = 10\n",
    "train_data = batchify(corpus.train, BATCH_SIZE)\n",
    "val_data = batchify(corpus.valid, EVAL_BATCH_SIZE)\n",
    "test_data = batchify(corpus.test, EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOKENS = len(corpus.dictionary)\n",
    "import model\n",
    "MODEL = \"LSTM\"\n",
    "EMSIZE = 200\n",
    "N_HID = 200\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "TIED = \"store_true\"\n",
    "LR = 20\n",
    "CLIP_TH = 0.25\n",
    "model = model.RNNModel(MODEL, N_TOKENS, EMSIZE, N_HID, N_LAYERS, DROPOUT, TIED).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPTT = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(BPTT, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |   200/ 2983 batches | lr 20.00 | ms/batch 19.42 | loss  7.62 | ppl  2034.00\n",
      "| epoch   0 |   400/ 2983 batches | lr 20.00 | ms/batch 16.20 | loss  6.80 | ppl   896.94\n",
      "| epoch   0 |   600/ 2983 batches | lr 20.00 | ms/batch 16.00 | loss  6.37 | ppl   586.04\n",
      "| epoch   0 |   800/ 2983 batches | lr 20.00 | ms/batch 15.92 | loss  6.20 | ppl   493.94\n",
      "| epoch   0 |  1000/ 2983 batches | lr 20.00 | ms/batch 15.85 | loss  6.05 | ppl   426.07\n",
      "| epoch   0 |  1200/ 2983 batches | lr 20.00 | ms/batch 15.88 | loss  5.96 | ppl   387.96\n",
      "| epoch   0 |  1400/ 2983 batches | lr 20.00 | ms/batch 15.95 | loss  5.85 | ppl   346.41\n",
      "| epoch   0 |  1600/ 2983 batches | lr 20.00 | ms/batch 15.82 | loss  5.85 | ppl   348.40\n",
      "| epoch   0 |  1800/ 2983 batches | lr 20.00 | ms/batch 15.52 | loss  5.70 | ppl   297.87\n",
      "| epoch   0 |  2000/ 2983 batches | lr 20.00 | ms/batch 15.51 | loss  5.66 | ppl   288.17\n",
      "| epoch   0 |  2200/ 2983 batches | lr 20.00 | ms/batch 15.88 | loss  5.56 | ppl   258.54\n",
      "| epoch   0 |  2400/ 2983 batches | lr 20.00 | ms/batch 16.04 | loss  5.56 | ppl   260.90\n",
      "| epoch   0 |  2600/ 2983 batches | lr 20.00 | ms/batch 16.00 | loss  5.55 | ppl   257.81\n",
      "| epoch   0 |  2800/ 2983 batches | lr 20.00 | ms/batch 16.14 | loss  5.44 | ppl   230.25\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import time\n",
    "import math\n",
    "criterion = nn.NLLLoss()\n",
    "model.train()\n",
    "total_loss = 0\n",
    "start_time = time.time()\n",
    "hidden = model.init_hidden(BATCH_SIZE)\n",
    "for epoch in range(1):\n",
    "    for batch, i in enumerate(range(0, train_data.size(0)-1, BPTT)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        model.zero_grad()\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), CLIP_TH)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(p.grad, alpha=-LR)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % 200 == 0 and batch > 0:\n",
    "            cur_loss = total_loss / 200\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // BPTT, LR,\n",
    "                elapsed * 1000 / 200, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write my own FNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNNModel(nn.Module):\n",
    "    def __init__(self, n_token, n_emb, n_hidden, seq_len):\n",
    "        super().__init__()\n",
    "        self.n_token = n_token\n",
    "        self.n_emb = n_emb \n",
    "        self.n_hidden = n_hidden\n",
    "        self.seq_len = seq_len\n",
    "        self.encoder = nn.Embedding(n_token, n_emb)\n",
    "        self.hidden = nn.Linear(n_emb*seq_len, n_hidden)\n",
    "        self.decoder = nn.Linear(n_hidden, n_token)\n",
    "    def forward(self, input):\n",
    "        emb = self.encoder(input)\n",
    "        batch_size = emb.shape[0]\n",
    "        emb = emb.view(batch_size, -1)\n",
    "        emb = torch.tanh(emb)\n",
    "        out = self.hidden(emb)\n",
    "        decoded = self.decoder(out)\n",
    "        decoded = nn.functional.log_softmax(decoded, dim=1)\n",
    "        return decoded\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Own Dataset\n",
    "class SequenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tensor_data, seq_len):\n",
    "        self.data = tensor_data\n",
    "        self.seq_len = seq_len\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len - 1\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i:i+self.seq_len], self.data[i+self.seq_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH = 1024\n",
    "EVAL_BATCH = 10000\n",
    "SEQ_LEN = 10\n",
    "train_loader = torch.utils.data.DataLoader(SequenceDataset(corpus.train, seq_len=SEQ_LEN), batch_size=TRAIN_BATCH)\n",
    "val_loader = torch.utils.data.DataLoader(SequenceDataset(corpus.valid, seq_len=SEQ_LEN), batch_size=EVAL_BATCH)\n",
    "test_loader = torch.utils.data.DataLoader(SequenceDataset(corpus.test, seq_len=SEQ_LEN), batch_size=EVAL_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FNNModel(\n",
    "    n_token=N_TOKENS,\n",
    "    n_emb=200,\n",
    "    n_hidden=200,\n",
    "    seq_len = SEQ_LEN\n",
    ")\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "        model.zero_grad()\n",
    "        out = model(X)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if i%100 == 0 and i > 0:\n",
    "            cur_loss = total_loss/100\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches |  ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, i, len(train_loader), \n",
    "                elapsed * 1000/100, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i, (X ,y) in enumerate(val_loader):\n",
    "            out = model(X)\n",
    "            loss = criterion(out, y)\n",
    "            total_loss += loss.item()\n",
    "        return total_loss/len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Starts\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   1 |   100/ 2040 batches |  ms/batch 204.57 | loss  7.91 | ppl  2727.57\n",
      "| epoch   1 |   200/ 2040 batches |  ms/batch 201.60 | loss  6.96 | ppl  1051.32\n",
      "| epoch   1 |   300/ 2040 batches |  ms/batch 198.83 | loss  6.88 | ppl   972.76\n",
      "| epoch   1 |   400/ 2040 batches |  ms/batch 205.47 | loss  6.80 | ppl   902.08\n",
      "| epoch   1 |   500/ 2040 batches |  ms/batch 196.52 | loss  6.59 | ppl   728.11\n",
      "| epoch   1 |   600/ 2040 batches |  ms/batch 200.35 | loss  6.56 | ppl   709.69\n",
      "| epoch   1 |   700/ 2040 batches |  ms/batch 197.50 | loss  6.53 | ppl   686.07\n",
      "| epoch   1 |   800/ 2040 batches |  ms/batch 199.97 | loss  6.41 | ppl   605.70\n",
      "| epoch   1 |   900/ 2040 batches |  ms/batch 200.21 | loss  6.43 | ppl   622.46\n",
      "| epoch   1 |  1000/ 2040 batches |  ms/batch 205.46 | loss  6.40 | ppl   599.25\n",
      "| epoch   1 |  1100/ 2040 batches |  ms/batch 202.07 | loss  6.38 | ppl   587.06\n",
      "| epoch   1 |  1200/ 2040 batches |  ms/batch 205.08 | loss  6.23 | ppl   508.83\n",
      "| epoch   1 |  1300/ 2040 batches |  ms/batch 205.67 | loss  6.31 | ppl   552.62\n",
      "| epoch   1 |  1400/ 2040 batches |  ms/batch 202.33 | loss  6.38 | ppl   592.16\n",
      "| epoch   1 |  1500/ 2040 batches |  ms/batch 202.99 | loss  6.24 | ppl   510.62\n",
      "| epoch   1 |  1600/ 2040 batches |  ms/batch 200.88 | loss  6.24 | ppl   513.74\n",
      "| epoch   1 |  1700/ 2040 batches |  ms/batch 203.18 | loss  6.34 | ppl   565.37\n",
      "| epoch   1 |  1800/ 2040 batches |  ms/batch 206.35 | loss  6.22 | ppl   502.90\n",
      "| epoch   1 |  1900/ 2040 batches |  ms/batch 207.95 | loss  6.18 | ppl   484.49\n",
      "| epoch   1 |  2000/ 2040 batches |  ms/batch 201.12 | loss  6.30 | ppl   542.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 423.25s | valid loss  5.90 | valid ppl   365.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   100/ 2040 batches |  ms/batch 201.53 | loss  5.88 | ppl   356.38\n",
      "| epoch   2 |   200/ 2040 batches |  ms/batch 204.01 | loss  5.65 | ppl   284.12\n",
      "| epoch   2 |   300/ 2040 batches |  ms/batch 199.16 | loss  5.74 | ppl   311.49\n",
      "| epoch   2 |   400/ 2040 batches |  ms/batch 198.40 | loss  5.78 | ppl   322.78\n",
      "| epoch   2 |   500/ 2040 batches |  ms/batch 198.28 | loss  5.63 | ppl   277.86\n",
      "| epoch   2 |   600/ 2040 batches |  ms/batch 204.11 | loss  5.65 | ppl   284.94\n",
      "| epoch   2 |   700/ 2040 batches |  ms/batch 206.13 | loss  5.62 | ppl   276.76\n",
      "| epoch   2 |   800/ 2040 batches |  ms/batch 202.08 | loss  5.56 | ppl   260.02\n",
      "| epoch   2 |   900/ 2040 batches |  ms/batch 200.79 | loss  5.59 | ppl   266.78\n",
      "| epoch   2 |  1000/ 2040 batches |  ms/batch 196.92 | loss  5.53 | ppl   252.54\n",
      "| epoch   2 |  1100/ 2040 batches |  ms/batch 200.28 | loss  5.59 | ppl   266.57\n",
      "| epoch   2 |  1200/ 2040 batches |  ms/batch 202.36 | loss  5.51 | ppl   247.24\n",
      "| epoch   2 |  1300/ 2040 batches |  ms/batch 198.34 | loss  5.55 | ppl   256.47\n",
      "| epoch   2 |  1400/ 2040 batches |  ms/batch 203.57 | loss  5.60 | ppl   271.48\n",
      "| epoch   2 |  1500/ 2040 batches |  ms/batch 199.95 | loss  5.52 | ppl   249.09\n",
      "| epoch   2 |  1600/ 2040 batches |  ms/batch 197.69 | loss  5.53 | ppl   253.24\n",
      "| epoch   2 |  1700/ 2040 batches |  ms/batch 195.48 | loss  5.63 | ppl   278.24\n",
      "| epoch   2 |  1800/ 2040 batches |  ms/batch 198.97 | loss  5.54 | ppl   254.69\n",
      "| epoch   2 |  1900/ 2040 batches |  ms/batch 197.64 | loss  5.50 | ppl   243.99\n",
      "| epoch   2 |  2000/ 2040 batches |  ms/batch 199.27 | loss  5.65 | ppl   283.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 418.74s | valid loss  5.78 | valid ppl   324.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   100/ 2040 batches |  ms/batch 199.81 | loss  5.38 | ppl   217.18\n",
      "| epoch   3 |   200/ 2040 batches |  ms/batch 198.45 | loss  5.13 | ppl   169.59\n",
      "| epoch   3 |   300/ 2040 batches |  ms/batch 200.22 | loss  5.24 | ppl   187.80\n",
      "| epoch   3 |   400/ 2040 batches |  ms/batch 197.97 | loss  5.23 | ppl   186.77\n",
      "| epoch   3 |   500/ 2040 batches |  ms/batch 197.30 | loss  5.12 | ppl   166.69\n",
      "| epoch   3 |   600/ 2040 batches |  ms/batch 200.39 | loss  5.13 | ppl   168.49\n",
      "| epoch   3 |   700/ 2040 batches |  ms/batch 200.51 | loss  5.10 | ppl   164.28\n",
      "| epoch   3 |   800/ 2040 batches |  ms/batch 197.62 | loss  5.05 | ppl   156.02\n",
      "| epoch   3 |   900/ 2040 batches |  ms/batch 202.30 | loss  5.08 | ppl   159.98\n",
      "| epoch   3 |  1000/ 2040 batches |  ms/batch 197.00 | loss  5.02 | ppl   151.36\n",
      "| epoch   3 |  1100/ 2040 batches |  ms/batch 201.15 | loss  5.08 | ppl   160.58\n",
      "| epoch   3 |  1200/ 2040 batches |  ms/batch 196.81 | loss  5.03 | ppl   152.69\n",
      "| epoch   3 |  1300/ 2040 batches |  ms/batch 197.10 | loss  5.05 | ppl   155.86\n",
      "| epoch   3 |  1400/ 2040 batches |  ms/batch 197.81 | loss  5.07 | ppl   159.26\n",
      "| epoch   3 |  1500/ 2040 batches |  ms/batch 199.38 | loss  5.04 | ppl   154.77\n",
      "| epoch   3 |  1600/ 2040 batches |  ms/batch 199.09 | loss  5.06 | ppl   157.74\n",
      "| epoch   3 |  1700/ 2040 batches |  ms/batch 197.17 | loss  5.14 | ppl   170.70\n",
      "| epoch   3 |  1800/ 2040 batches |  ms/batch 196.87 | loss  5.07 | ppl   158.65\n",
      "| epoch   3 |  1900/ 2040 batches |  ms/batch 199.94 | loss  5.03 | ppl   153.22\n",
      "| epoch   3 |  2000/ 2040 batches |  ms/batch 198.75 | loss  5.17 | ppl   176.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 415.79s | valid loss  5.83 | valid ppl   341.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   100/ 2040 batches |  ms/batch 200.65 | loss  4.99 | ppl   146.70\n",
      "| epoch   4 |   200/ 2040 batches |  ms/batch 198.83 | loss  4.76 | ppl   117.15\n",
      "| epoch   4 |   300/ 2040 batches |  ms/batch 198.28 | loss  4.85 | ppl   128.21\n",
      "| epoch   4 |   400/ 2040 batches |  ms/batch 199.65 | loss  4.85 | ppl   127.13\n",
      "| epoch   4 |   500/ 2040 batches |  ms/batch 196.12 | loss  4.76 | ppl   116.84\n",
      "| epoch   4 |   600/ 2040 batches |  ms/batch 198.99 | loss  4.76 | ppl   116.48\n",
      "| epoch   4 |   700/ 2040 batches |  ms/batch 198.82 | loss  4.73 | ppl   113.65\n",
      "| epoch   4 |   800/ 2040 batches |  ms/batch 200.18 | loss  4.69 | ppl   108.62\n",
      "| epoch   4 |   900/ 2040 batches |  ms/batch 201.56 | loss  4.70 | ppl   110.18\n",
      "| epoch   4 |  1000/ 2040 batches |  ms/batch 202.38 | loss  4.67 | ppl   106.36\n",
      "| epoch   4 |  1100/ 2040 batches |  ms/batch 203.37 | loss  4.72 | ppl   112.13\n",
      "| epoch   4 |  1200/ 2040 batches |  ms/batch 203.53 | loss  4.69 | ppl   108.80\n",
      "| epoch   4 |  1300/ 2040 batches |  ms/batch 200.96 | loss  4.70 | ppl   109.78\n",
      "| epoch   4 |  1400/ 2040 batches |  ms/batch 204.34 | loss  4.70 | ppl   109.63\n",
      "| epoch   4 |  1500/ 2040 batches |  ms/batch 202.71 | loss  4.71 | ppl   111.22\n",
      "| epoch   4 |  1600/ 2040 batches |  ms/batch 201.40 | loss  4.73 | ppl   112.99\n",
      "| epoch   4 |  1700/ 2040 batches |  ms/batch 201.78 | loss  4.79 | ppl   120.84\n",
      "| epoch   4 |  1800/ 2040 batches |  ms/batch 203.66 | loss  4.72 | ppl   112.33\n",
      "| epoch   4 |  1900/ 2040 batches |  ms/batch 203.58 | loss  4.70 | ppl   110.31\n",
      "| epoch   4 |  2000/ 2040 batches |  ms/batch 203.56 | loss  4.83 | ppl   124.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 420.97s | valid loss  5.92 | valid ppl   373.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   100/ 2040 batches |  ms/batch 200.00 | loss  4.70 | ppl   109.56\n",
      "| epoch   5 |   200/ 2040 batches |  ms/batch 200.64 | loss  4.49 | ppl    89.12\n",
      "| epoch   5 |   300/ 2040 batches |  ms/batch 200.75 | loss  4.58 | ppl    97.05\n",
      "| epoch   5 |   400/ 2040 batches |  ms/batch 199.31 | loss  4.57 | ppl    96.78\n",
      "| epoch   5 |   500/ 2040 batches |  ms/batch 202.81 | loss  4.51 | ppl    91.21\n",
      "| epoch   5 |   600/ 2040 batches |  ms/batch 204.50 | loss  4.50 | ppl    89.82\n",
      "| epoch   5 |   700/ 2040 batches |  ms/batch 199.35 | loss  4.47 | ppl    87.40\n",
      "| epoch   5 |   800/ 2040 batches |  ms/batch 211.86 | loss  4.44 | ppl    84.41\n",
      "| epoch   5 |   900/ 2040 batches |  ms/batch 201.29 | loss  4.44 | ppl    84.58\n",
      "| epoch   5 |  1000/ 2040 batches |  ms/batch 205.12 | loss  4.42 | ppl    83.30\n",
      "| epoch   5 |  1100/ 2040 batches |  ms/batch 206.36 | loss  4.46 | ppl    86.88\n",
      "| epoch   5 |  1200/ 2040 batches |  ms/batch 202.14 | loss  4.45 | ppl    85.68\n",
      "| epoch   5 |  1300/ 2040 batches |  ms/batch 201.92 | loss  4.45 | ppl    85.53\n",
      "| epoch   5 |  1400/ 2040 batches |  ms/batch 206.97 | loss  4.43 | ppl    83.94\n",
      "| epoch   5 |  1500/ 2040 batches |  ms/batch 202.48 | loss  4.47 | ppl    87.50\n",
      "| epoch   5 |  1600/ 2040 batches |  ms/batch 200.47 | loss  4.49 | ppl    88.82\n",
      "| epoch   5 |  1700/ 2040 batches |  ms/batch 202.66 | loss  4.54 | ppl    93.92\n",
      "| epoch   5 |  1800/ 2040 batches |  ms/batch 202.02 | loss  4.47 | ppl    87.18\n",
      "| epoch   5 |  1900/ 2040 batches |  ms/batch 199.71 | loss  4.46 | ppl    86.40\n",
      "| epoch   5 |  2000/ 2040 batches |  ms/batch 200.90 | loss  4.57 | ppl    96.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 423.39s | valid loss  6.01 | valid ppl   408.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   100/ 2040 batches |  ms/batch 204.29 | loss  4.49 | ppl    88.73\n",
      "| epoch   6 |   200/ 2040 batches |  ms/batch 203.98 | loss  4.29 | ppl    73.07\n",
      "| epoch   6 |   300/ 2040 batches |  ms/batch 198.25 | loss  4.37 | ppl    79.30\n",
      "| epoch   6 |   400/ 2040 batches |  ms/batch 202.37 | loss  4.37 | ppl    79.39\n",
      "| epoch   6 |   500/ 2040 batches |  ms/batch 202.29 | loss  4.34 | ppl    76.44\n",
      "| epoch   6 |   600/ 2040 batches |  ms/batch 203.21 | loss  4.31 | ppl    74.47\n",
      "| epoch   6 |   700/ 2040 batches |  ms/batch 202.28 | loss  4.28 | ppl    72.36\n",
      "| epoch   6 |   800/ 2040 batches |  ms/batch 204.31 | loss  4.25 | ppl    70.46\n",
      "| epoch   6 |   900/ 2040 batches |  ms/batch 202.36 | loss  4.25 | ppl    70.01\n",
      "| epoch   6 |  1000/ 2040 batches |  ms/batch 200.61 | loss  4.25 | ppl    69.96\n",
      "| epoch   6 |  1100/ 2040 batches |  ms/batch 200.93 | loss  4.28 | ppl    72.21\n",
      "| epoch   6 |  1200/ 2040 batches |  ms/batch 196.98 | loss  4.28 | ppl    72.09\n",
      "| epoch   6 |  1300/ 2040 batches |  ms/batch 203.29 | loss  4.27 | ppl    71.52\n",
      "| epoch   6 |  1400/ 2040 batches |  ms/batch 200.75 | loss  4.24 | ppl    69.15\n",
      "| epoch   6 |  1500/ 2040 batches |  ms/batch 201.01 | loss  4.29 | ppl    73.32\n",
      "| epoch   6 |  1600/ 2040 batches |  ms/batch 201.26 | loss  4.31 | ppl    74.41\n",
      "| epoch   6 |  1700/ 2040 batches |  ms/batch 205.01 | loss  4.36 | ppl    78.11\n",
      "| epoch   6 |  1800/ 2040 batches |  ms/batch 199.56 | loss  4.28 | ppl    72.29\n",
      "| epoch   6 |  1900/ 2040 batches |  ms/batch 198.65 | loss  4.28 | ppl    72.10\n",
      "| epoch   6 |  2000/ 2040 batches |  ms/batch 202.82 | loss  4.38 | ppl    79.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 422.12s | valid loss  6.10 | valid ppl   444.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   100/ 2040 batches |  ms/batch 205.71 | loss  4.33 | ppl    75.78\n",
      "| epoch   7 |   200/ 2040 batches |  ms/batch 201.70 | loss  4.14 | ppl    62.85\n",
      "| epoch   7 |   300/ 2040 batches |  ms/batch 207.50 | loss  4.22 | ppl    68.08\n",
      "| epoch   7 |   400/ 2040 batches |  ms/batch 203.92 | loss  4.23 | ppl    68.50\n",
      "| epoch   7 |   500/ 2040 batches |  ms/batch 203.57 | loss  4.20 | ppl    66.73\n",
      "| epoch   7 |   600/ 2040 batches |  ms/batch 207.43 | loss  4.17 | ppl    64.43\n",
      "| epoch   7 |   700/ 2040 batches |  ms/batch 207.05 | loss  4.14 | ppl    62.71\n",
      "| epoch   7 |   800/ 2040 batches |  ms/batch 207.19 | loss  4.12 | ppl    61.33\n",
      "| epoch   7 |   900/ 2040 batches |  ms/batch 206.49 | loss  4.10 | ppl    60.46\n",
      "| epoch   7 |  1000/ 2040 batches |  ms/batch 205.09 | loss  4.11 | ppl    61.11\n",
      "| epoch   7 |  1100/ 2040 batches |  ms/batch 202.54 | loss  4.13 | ppl    62.38\n",
      "| epoch   7 |  1200/ 2040 batches |  ms/batch 202.34 | loss  4.14 | ppl    62.99\n",
      "| epoch   7 |  1300/ 2040 batches |  ms/batch 203.33 | loss  4.13 | ppl    62.17\n",
      "| epoch   7 |  1400/ 2040 batches |  ms/batch 206.78 | loss  4.09 | ppl    59.45\n",
      "| epoch   7 |  1500/ 2040 batches |  ms/batch 205.58 | loss  4.16 | ppl    63.90\n",
      "| epoch   7 |  1600/ 2040 batches |  ms/batch 201.20 | loss  4.17 | ppl    64.84\n",
      "| epoch   7 |  1700/ 2040 batches |  ms/batch 205.55 | loss  4.21 | ppl    67.63\n",
      "| epoch   7 |  1800/ 2040 batches |  ms/batch 203.79 | loss  4.14 | ppl    62.52\n",
      "| epoch   7 |  1900/ 2040 batches |  ms/batch 203.18 | loss  4.14 | ppl    62.52\n",
      "| epoch   7 |  2000/ 2040 batches |  ms/batch 203.88 | loss  4.23 | ppl    68.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 428.06s | valid loss  6.17 | valid ppl   480.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "Early Stop at epoch 7\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = 'model.pt'\n",
    "print(\"Training Starts\")\n",
    "\n",
    "best_val_loss = None\n",
    "count = 0\n",
    "for epoch in range(1, 50+1):\n",
    "    print(\"-\"*89)\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate()\n",
    "    print(\"-\"*89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        with open(SAVE_DIR, 'wb') as f:\n",
    "            torch.save(model, f)\n",
    "        best_val_loss = val_loss\n",
    "        count = 0\n",
    "    else:\n",
    "        count += 1\n",
    "    if count >= 5:\n",
    "        print(f'Early Stop at epoch {epoch}')\n",
    "        break \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| End of training | test loss  5.70 | test ppl   297.93\n"
     ]
    }
   ],
   "source": [
    "with open(SAVE_DIR, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i, (X ,y) in enumerate(test_loader):\n",
    "            out = model(X)\n",
    "            loss = criterion(out, y)\n",
    "            total_loss += loss.item()\n",
    "        test_loss = total_loss/len(test_loader)\n",
    "    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, it overfits the tranining data. Train agian with drop out layer and smaller length of sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python TrainWithDropOut.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'N_TOKENS' from 'TrainWithDropOut' (c:\\Users\\HONGYI001\\Desktop\\NLP Assignment1\\examples\\word_language_model\\TrainWithDropOut.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\HONGYI~1\\AppData\\Local\\Temp/ipykernel_9868/1190403647.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mTrainWithDropOut\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFNNModelDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mTrainWithDropOut\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mN_TOKENS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSEQ_LEN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDROPOUT_P\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m model = FNNModelDropout(\n\u001b[0;32m      4\u001b[0m     \u001b[0mn_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mN_TOKENS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mn_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'N_TOKENS' from 'TrainWithDropOut' (c:\\Users\\HONGYI001\\Desktop\\NLP Assignment1\\examples\\word_language_model\\TrainWithDropOut.py)"
     ]
    }
   ],
   "source": [
    "from TrainWithDropOut import FNNModelDropout\n",
    "from TrainWithDropOut import N_TOKENS, SEQ_LEN, DROPOUT_P\n",
    "model = FNNModelDropout(\n",
    "    n_token=N_TOKENS,\n",
    "    n_emb = 200,\n",
    "    n_hidden=200,\n",
    "    seq_len=SEQ_LEN,\n",
    "    dropout_p= DROPOUT_P\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'FNNModelDropout' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\HONGYI~1\\AppData\\Local\\Temp/ipykernel_26400/3095515933.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model_dropout.pt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VFL\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    605\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VFL\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m    880\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 882\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VFL\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mfind_class\u001b[1;34m(self, mod_name, name)\u001b[0m\n\u001b[0;32m    873\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfind_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m             \u001b[0mmod_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_module_mapping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 875\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m     \u001b[1;31m# Load the data (which may in turn use `persistent_load` to load tensors)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't get attribute 'FNNModelDropout' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "model \n",
    "with open('model_dropout.pt', 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i, (X ,y) in enumerate(test_loader):\n",
    "            out = model(X)\n",
    "            loss = criterion(out, y)\n",
    "            total_loss += loss.item()\n",
    "        test_loss = total_loss/len(test_loader)\n",
    "    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1579d4a348c2ef16482c05d3cfac916f73c8945ddf1938a1e045b3bdea82eece"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('VFL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
